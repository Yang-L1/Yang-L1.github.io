<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yang Li</title>

    <meta name="author" content="Yang Li（李楊）">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <meta name="google-site-verification" content="LJmwDttzihdzxs3PUY_YWZrKU8ZlcniRfISDzdJlXtU" />
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Yang Li
                </p>
                <p>
                  I'm a research scientist at Tencent (Hunyuan3D).
                </p>
                <p>
                  I finished my Ph.D. with  <a href="https://www.mi.t.u-tokyo.ac.jp/harada/">Tatsuya Harada</a> at The University of Tokyo.
                  During my Ph.D., I did an internship at Technical University Munich with <a href="https://niessnerlab.org/members/matthias_niessner/profile.html">Matthias Nießner</a>,
                  and an internship with <a href="http://www.bozheng-lab.com/">Bo Zheng</a> at Huawei Japan Research Center.
		  I did a master in bioinformatics with <a href="https://shibuyalab.hgc.jp//">Tetsuo Shibuya</a> at The University of Tokyo.
                </p>
                <p>
                  My research interests lie in the intersection of 3D computer vision, artificial intelligence, particularly focusing on registration, 3D/4D reconstruction, 3D Generative modeling,
                  with applications in VR/AR, robotics, etc.
                </p>

<!--                 <p>
                  <font color=#ff4500><strong>我们在上海有一些关于3D AIGC的研究实习生岗位，欢迎联系我们！</strong></font>

                </p>
 -->
                <p style="text-align:center">
                  <a href="mailto:liyang@mi.t.u-tokyo.ac.jp">Email</a> &nbsp;/&nbsp;
<!--                  <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp;-->
<!--                  <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp;-->
                  <a href="https://scholar.google.co.jp/citations?user=ECzmAC8AAAAJ">Scholar</a> &nbsp;/&nbsp;
<!--				  <a href="https://www.threads.net/@jonbarron">Threads</a> &nbsp;/&nbsp;-->
<!--				  <a href="https://bsky.app/profile/jonbarron.bsky.social">Bluesky</a> &nbsp;/&nbsp;-->
<!--                  <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp;-->
                  <a href="https://github.com/rabbityl/">Github</a>
                </p>



              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/rabbit.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/rabbit.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>

              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>



            <!-- phycage -->
              <tr onmouseout="_3dseg_stop()" onmouseover="_3dseg_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
<!--                    <a href="https://wolfball.github.io/frankenstein">-->
                      <div class="two" id="3dseg_image" style="opacity: 0;">
                        <img src="main_page/images/phycage.jpg" width="170">
                      </div>
                      <img src="main_page/images/phycage.jpg" width="170">
<!--                    </a>-->
                  </div>

                  <script type="text/javascript">
                    function _3dseg_start() {
                      document.getElementById('3dseg_image').style.opacity = 1;
                    }
                    function _3dseg_stop() {
                      document.getElementById('3dseg_image').style.opacity = 0;
                    }
                    optnrt_stop();
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <a href="https://wolfball.github.io/frankenstein">
                    <papertitle>
                    PhyCAGE: Physically Plausible Compositional 3D Asset Generation from a Single Image
                  </a>
                  <br>
                  Han Yan, Mingrui Zhang, <b>Yang Li</b>, Chao Ma, Pan Ji
                  <br>
                  <b>Arxiv preprint 2024 </b><br>
                  <a href="https://wolfball.github.io/phycage">Project Page</a>|
                  <a href="https://arxiv.org/abs/2411.18548">Paper</a>|
                  <a href="https://youtu.be/IHQqLli_s2o">video</a>
                  <p>
                 PhyCAGE generates physically plausible compositional 3D assets from a single image.

                  </p>

                </td>
              </tr>


              <!-- Lam3D -->
              <tr onmouseout="_3dseg_stop()" onmouseover="_3dseg_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
<!--                    <a href="https://wolfball.github.io/frankenstein">-->
                      <div class="two" id="3dseg_image" style="opacity: 0;">
                        <img src="main_page/images/lam3d.png" width="170">
                      </div>
                      <img src="main_page/images/lam3d.png" width="170">
<!--                    </a>-->
                  </div>

                  <script type="text/javascript">
                    function _3dseg_start() {
                      document.getElementById('3dseg_image').style.opacity = 1;
                    }
                    function _3dseg_stop() {
                      document.getElementById('3dseg_image').style.opacity = 0;
                    }
                    optnrt_stop();
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <a href="https://arxiv.org/abs/2405.15622">
                    <papertitle>
LAM3D: Large Image-Point-Cloud Alignment Model for 3D Reconstruction from Single Image
                  </a>
                  <br>
                  Ruikai Cui, Xibin Song, Weixuan Sun, Senbo Wang, Weizhe Liu, Shenzhou Chen, Taizhang Shang, <b>Yang Li</b>, Nick Barnes, Hongdong Li, Pan Ji
                  <br>
                  <b>NeurIPS 2024 </b><br>
<!--                  <a href="https://wolfball.github.io/phycage">Project Page</a>|-->
                  <a href="https://arxiv.org/abs/2405.15622">Paper</a>
<!--                  <a href="https://youtu.be/IHQqLli_s2o">video</a>-->
                  <p>
We introduce a novel framework, the Large Image and Point Cloud Alignment Model (LAM3D), which utilizes 3D point cloud data to enhance the fidelity of generated 3D meshes.

                  </p>

                </td>
              </tr>

              <!-- Sketch2Scene -->
              <tr onmouseout="_3dseg_stop()" onmouseover="_3dseg_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <a href="https://xrvisionlabs.github.io/Sketch2Scene/">
                      <div class="two" id="3dseg_image" style="opacity: 0;">
                        <img src="main_page/images/s2scene.jpg" width="170">
                      </div>
                      <img src="main_page/images/s2scene.jpg" width="170">
                    </a>
                  </div>

                  <script type="text/javascript">
                    function _3dseg_start() {
                      document.getElementById('3dseg_image').style.opacity = 1;
                    }
                    function _3dseg_stop() {
                      document.getElementById('3dseg_image').style.opacity = 0;
                    }
                    optnrt_stop();
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <a href="https://xrvisionlabs.github.io/Sketch2Scene/">
                    <papertitle>
                      Sketch2Scene: Automatic Generation of Interactive 3D Game Scenes from User's Casual Sketches
                  </a>
                  <br>
                  Yongzhi Xu, Yonhon Ng, Yifu Wang, Inkyu Sa, Yunfei Duan, <b>Yang Li</b>, Pan Ji, Hongdong Li
                  <br>
                  <b>Arxiv preprint 2024 </b><br>
                  <a href="https://xrvisionlabs.github.io/Sketch2Scene/">Project Page</a>|
                  <a href="https://arxiv.org/abs/2408.04567">Paper</a>|
                  <a href="https://www.youtube.com/watch?v=j1jZ-An_uoc">video</a>
                  <p>
                    This paper proposes a novel approach for automatically generating interactive (i.e., playable) 3D game scenes from users' casual prompts, including hand-drawn sketches and text descriptions.
                  </p>

                </td>
              </tr>



            <!-- han sigais -->
              <tr onmouseout="_3dseg_stop()" onmouseover="_3dseg_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <a href="https://wolfball.github.io/frankenstein">
                      <div class="two" id="3dseg_image" style="opacity: 0;">
                        <img src="main_page/images/han24.png" width="170">
                      </div>
                      <img src="main_page/images/han24.png" width="170">
                    </a>
                  </div>

                  <script type="text/javascript">
                    function _3dseg_start() {
                      document.getElementById('3dseg_image').style.opacity = 1;
                    }
                    function _3dseg_stop() {
                      document.getElementById('3dseg_image').style.opacity = 0;
                    }
                    optnrt_stop();
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <a href="https://wolfball.github.io/frankenstein">
                    <papertitle>
                      Frankenstein: Generating Semantic-Compositional 3D Scenes in One Tri-Plane
                  </a>
                  <br>
                  Han Yan, <b>Yang Li</b>, Zhennan Wu, Shenzhou Chen, Weixuan Sun, Taizhang Shang, Weizhe Liu, Tian Chen, Xiaqiang Dai, Chao Ma, Hongdong Li, Pan Ji
                  <br>
                  <b>SIGGRAPH Asia 2024 </b><br>
                  <a href="https://wolfball.github.io/frankenstein">Project Page</a>|
                  <a href="https://arxiv.org/abs/2403.16210">Paper</a>|
                  <a href="https://www.youtube.com/watch?v=lRn-HqyCrLI">video</a>|
                  <a href="https://github.com/Tencent/Frankenstein">code</a>

                  <p>
                  We present Frankenstein, a diffusion-based framework that can
                  generate semantic-compositional 3D scenes in a single pass. Unlike existing
                  methods that output a single, unified 3D shape, Frankenstein simultaneously
                  generates multiple separated shapes, each corresponding to a semantically
                  meaningful part.

                  </p>

                </td>
              </tr>

          <!-- Blockfusion -->
              <tr onmouseout="_3dseg_stop()" onmouseover="_3dseg_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <a href="https://yang-l1.github.io/blockfusion">
                      <div class="two" id="3dseg_image" style="opacity: 0;">
                        <img src="blockfusion/img/icon.png" width="160">
                      </div>
                      <img src="blockfusion/img/icon.png" width="160">
                    </a>
                  </div>

                  <script type="text/javascript">
                    function _3dseg_start() {
                      document.getElementById('3dseg_image').style.opacity = 1;
                    }
                    function _3dseg_stop() {
                      document.getElementById('3dseg_image').style.opacity = 0;
                    }
                    optnrt_stop();
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <a href="https://yang-l1.github.io/blockfusion/">
                    <papertitle>
                     BlockFusion: Expandable 3D Scene Generation using Latent Tri-plane Extrapolation                  </a>
                  <br>

                   Zhennan Wu, <b> Yang Li <strong style="color:#d54322;">†</strong></b> , Han Yan,  Taizhang Shang,  Weixuan Sun,  Senbo Wang,  Ruikai Cui, Weizhe Liu,  Hiroyuki Sato, Hongdong Li, and  Pan Ji
                  <br>
                  <b>Transaction on Graphics 2024 </b>
                  &nbsp <font color="red"><strong>(Selected as <a href="https://www.youtube.com/watch?v=tjYVcOJONdI&t=188s" style="color:#FF0000;text-decoration: underline">SIGGRAPH 2024 Trailer Video</a>)</strong></font>
                  <br>
	    	  <a href="https://yang-l1.github.io/blockfusion/">Project Page</a>|
                  <a href="https://arxiv.org/abs/2401.17053">Paper</a>|
                  <a href="https://www.youtube.com/watch?v=PxIBtd6G0mA">video</a>|
                  <a href="https://github.com/Tencent/BlockFusion/">Code</a>



                  <p>
			  We introduce the first 3D diffusion based approach for directly generating large unbounded 3D scene in both inddor and outdoor scenarios.
			  At the core of this approach is a novel tri-plane diffusion and tri-plane extrapolation mechanism.
                  </p>

                </td>
            </tr>






            <!-- ruikai eccv -->
              <tr onmouseout="_3dseg_stop()" onmouseover="_3dseg_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <a href="https://weizheliu.github.io/NeuSDFusion/">
                      <div class="two" id="3dseg_image" style="opacity: 0;">
                        <img src="main_page/images/eccv24.png" width="160">
                      </div>
                      <img src="main_page/images/eccv24.png" width="160">
                    </a>
                  </div>

                  <script type="text/javascript">
                    function _3dseg_start() {
                      document.getElementById('3dseg_image').style.opacity = 1;
                    }
                    function _3dseg_stop() {
                      document.getElementById('3dseg_image').style.opacity = 0;
                    }
                    optnrt_stop();
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <a href="https://weizheliu.github.io/NeuSDFusion/">
                    <papertitle>
                    NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion, Reconstruction, and Generation
                  </a>
                  <br>
                  Ruikai Cui, Weizhe Liu, Weixuan Sun, Senbo Wang, Taizhang Shang,<b> Yang Li </b>, Xibin Song, Han Yan, Zhennan Wu, Shenzhou Chen, Hongdong Li, Pan Ji
                  <br>
                  <b>ECCV 2024</b><br>
	    	      <a href="https://weizheliu.github.io/NeuSDFusion/">Project Page</a>|
                  <a href="https://arxiv.org/abs/2403.18241">Paper</a>
                  <p>
In this pa- per, we introduce a novel spatial-aware 3D shape generation framework that leverages 2D plane representations for enhanced 3D shape modeling.
                  </p>

                </td>
              </tr>


             <!-- 3dsegmentor -->
              <tr onmouseout="_3dseg_stop()" onmouseover="_3dseg_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    
                      <div class="two" id="3dseg_image" style="opacity: 0;">
                        <img src="main_page/images/3dseg.png" width="160">
                      </div>
                      <img src="main_page/images/3dseg.png" width="160">
                    
                  </div>

                  <script type="text/javascript">
                    function _3dseg_start() {
                      document.getElementById('3dseg_image').style.opacity = 1;
                    }
                    function _3dseg_stop() {
                      document.getElementById('3dseg_image').style.opacity = 0;
                    }
                    optnrt_stop();
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <a href="https://openreview.net/forum?id=4dZeBJ83oxk">
                    <papertitle>
                     3D Segmenter: 3D Transformer based Semantic Segmentation via 2D Panoramic Distillation                   </papertitle>
                  </a>
                  <br>

                  Zhennan Wu, <b>Yang Li</b> , Yifei Huang, Lin Gu, Tatsuya Harada, and Hiroyuki Sato
                  <br>
                  <b>ICLR 2023 </b><br>
                  <a href="https://openreview.net/forum?id=4dZeBJ83oxk">paper</a>|
<!--                  <a href="https://www.youtube.com/watch?v=IdLHbru6G8s">video</a>|-->
                  <a href="https://github.com/swwzn714/3DSegmenter">Code</a>

                  <p>
                  We propose the first 2D-to-3D knowledge distillation strategy to enhance 3D semantic segmentation model with knowledge embedded in the latent space of powerful 2D models.
                  </p>

                </td>
              </tr>






             <!-- ndp -->
              <tr onmouseout="ndp_stop()" onmouseover="ndp_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                     
                      <div class="two" id="ndp_image" style="opacity: 0;">
                        <img src="main_page/images/ndp.png" width="160">
                      </div>
                      <img src="main_page/images/ndp.png" width="160">
                    
                  </div>

                  <script type="text/javascript">
                    function ndp_start() {
                      document.getElementById('ndp_image').style.opacity = 1;
                    }
                    function ndp_stop() {
                      document.getElementById('ndp_image').style.opacity = 0;
                    }
                    optnrt_stop();
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <a href="https://yang-l1.github.io">
                    <papertitle>
                      Non-rigid Point Cloud Registration with Neural Deformation Pyramid </papertitle>
                  </a>
                  <br>
                  <b>Yang Li</b> and Tatsuya Harada
                  <br>
                  <b>NeurIPS 2022 </b><br>
                  <a href="https://arxiv.org/abs/2205.12796">paper</a>|
<!--                  <a href="https://www.youtube.com/watch?v=IdLHbru6G8s">video</a>|-->
                  <a href="https://github.com/rabbityl/DeformationPyramid">Code</a>

                  <p>
                     Neural Deformation Pyramid (NDP) break down non-rigid point cloud registration problem via hierarchical motion decomposition.
                    NDP demonstrates advantage in both speed and registration accuracy.
                  </p>

                </td>
              </tr>





             <!-- lepard -->
              <tr onmouseout="lepard_stop()" onmouseover="lepard_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                     
                      <div class="two" id="lepard_image" style="opacity: 0;">
                        <img src="main_page/images/lepard.png" width="160">
                      </div>
                      <img src="main_page/images/lepard.png" width="160">
                    
                  </div>

                  <script type="text/javascript">
                    function lepard_start() {
                      document.getElementById('lepard_image').style.opacity = 1;
                    }
                    function lepard_stop() {
                      document.getElementById('lepard_image').style.opacity = 0;
                    }
                    optnrt_stop();
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <a href="https://yang-l1.github.io">
                    <papertitle>
                     Lepard: Learning partial point cloud matching in rigid and deformable scenes                  </papertitle>
                  </a>
                  <br>
                  <b>Yang Li</b> and Tatsuya Harada
                  <br>
                  <b>CVPR 2022 </b>
                  &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                  <br>
                  <a href="https://arxiv.org/abs/2111.12591">paper</a>|
                  <a href="https://www.youtube.com/watch?v=IdLHbru6G8s">video</a>|
                  <a href="https://github.com/rabbityl/lepard">Code</a>

                  <p>
                  We design Lepard, a novel partial point clouds matching method that exploits 3D positional knowledge.
                    Lepard reaches SOTA on both rigid and deformable point cloud matching benchmarks.
                  </p>

                </td>
              </tr>




             <!-- 4dcomplete -->
              <tr onmouseout="spltfusion_stop()" onmouseover="spltfusion_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                      <div class="two" id="4dcomplete_image" style="opacity: 0;">
                        <img src="main_page/images/4dcomplete.png" width="160">
                      </div>
                      <img src="main_page/images/4dcomplete.png" width="160">
                    
                  </div>

                  <script type="text/javascript">
                    function spltfusion_start() {
                      document.getElementById('4dcomplete_image').style.opacity = 1;
                    }
                    function spltfusion_stop() {
                      document.getElementById('4dcomplete_image').style.opacity = 0;
                    }
                    optnrt_stop();
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <a href="https://yang-l1.github.io">
                    <papertitle>
                    4DComplete: Non-Rigid Motion Estimation Beyond the Observable Surface                    </papertitle>
                  </a>
                  <br>
                  <b>Yang Li</b>, Hiraki Takehara, Takafumi Taketomi, Bo Zheng, and Matthias Nießner
                  <br>
                  <b>ICCV 2021</b><br>
                  <a href="https://arxiv.org/abs/2105.01905">paper</a>|
                  <a href="https://www.youtube.com/watch?v=QrSsVoTRpWk">video</a>|
                  <a href="ttps://github.com/rabbityl/DeformingThings4D">Code</a>

                  <p> We introduce 4DComplete, the first method that jointly recovers the shape and motion field from partial observations. We also provide a large-scale non-rigid 4D dataset for training and benchmaring. It consists of 1,972 animation sequences, and 122,365 frames.
                  </p>

                </td>
              </tr>


             <!-- spltfusion -->
              <tr onmouseout="spltfusion_stop()" onmouseover="spltfusion_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                      <div class="two" id="spltfusion_image" style="opacity: 0;">
                        <img src="main_page/images/splitfusion.png" width="160">
                      </div>
                      <img src="main_page/images/splitfusion.png" width="160">
                  </div>

                  <script type="text/javascript">
                    function spltfusion_start() {
                      document.getElementById('spltfusion_image').style.opacity = 1;
                    }
                    function spltfusion_stop() {
                      document.getElementById('spltfusion_image').style.opacity = 0;
                    }
                    optnrt_stop();
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <a href="https://yang-l1.github.io">
                    <papertitle>SplitFusion: Simultaneous Tracking and Mapping for Non-Rigid Scenes</papertitle>
                  </a>
                  <br>
                  <b>Yang Li</b>, Tianwei Zhang, Yoshihiko Nakamura, and Tatsuya Harada
                  <br>
                  <b>IROS 2020</b><br>
                  <a href="https://arxiv.org/abs/2007.02108">paper</a>|
                  <a href="https://www.youtube.com/watch?v=HVjdS7E8N94">video</a>

                  <p> SplitFusion is a dense RGB-D SLAM framework that simultaneously performs tracking and dense reconstruction for both rigid and non-rigid components of the scene.
                  </p>

                </td>
              </tr>




             <!-- Learning to Optimize Non-Rigid Tracking -->
              <tr onmouseout="optnrt_stop()" onmouseover="optnrt_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                      <div class="two" id="optnrt_image" style="opacity: 0;">
                        <img src="main_page/images/li2020optnrt.png" width="160">
                      </div>
                      <img src="main_page/images/li2020optnrt.png" width="160">
                  </div>

                  <script type="text/javascript">
                    function optnrt_start() {
                      document.getElementById('optnrt_image').style.opacity = 1;
                    }
                    function optnrt_stop() {
                      document.getElementById('optnrt_image').style.opacity = 0;
                    }
                    optnrt_stop();
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <a href="http://niessnerlab.org/projects/li2020learning.html">
                    <papertitle>Learning to Optimize Non-Rigid Tracking</papertitle>
                  </a>
                  <br>

                  <b>Yang Li</b>, Aljaž Božič, Tianwei Zhang, Yanli Ji, Tatsuya Harada, and Matthias Nießner<br>
                  <b>CVPR 2020</b>
                                    &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                  <br>

                  <a href="https://arxiv.org/pdf/2003.12230.pdf">paper</a> |
                  <a href="https://www.youtube.com/watch?v=oFgMep7xzrU">video</a>

                  <p>
                    We learn the tracking of non-rigid objects by differentiating through the underlying non-rigid
                    solver. Specifically, we propose ConditionNet which learns to generate a problem-specific
                    preconditioner using a large number of training samples from the Gauss-Newton update equation. The
                    learned preconditioner increases PCG’s convergence speed by a significant margin.
                  </p>

                </td>
              </tr>
	


             <!-- flowfusion -->
              <tr onmouseout="flowfusion_stop()" onmouseover="flowfusion_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                      <div class="two" id="flowfusion_image" style="opacity: 0;">
                        <img src="main_page/images/flowfusion.png" width="160">
                      </div>
                      <img src="main_page/images/flowfusion.png" width="160">
                  </div>

                  <script type="text/javascript">
                    function flowfusion_start() {
                      document.getElementById('flowfusion_image').style.opacity = 1;
                    }
                    function flowfusion_stop() {
                      document.getElementById('flowfusion_image').style.opacity = 0;
                    }
                    optnrt_stop();
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <a href="https://yang-l1.github.io">
                    <papertitle>FlowFusion: Dynamic Dense RGB-D SLAM Based on Optical Flow</papertitle>
                  </a>
                  <br>
                  Tianwei Zhang, Huayan Zhang, <b>Yang Li</b>, Yoshihiko Nakamura, and Lei Zhang<br>
                  <b>ICRA 2020</b><br>
                  <a href="https://arxiv.org/pdf/1903.06315.pdf">paper</a>|
                  <a href="https://www.youtube.com/watch?v=6yPGDdwKFLA">video</a>

                  <p>
                  We present a novel dense RGB-D SLAM solution that simultaneously accomplishes the dynamic/static segmentation and camera ego-motion estimation as well as the static background reconstructions.
                  </p>

                </td>
              </tr>


             <!-- icra19 -->
              <tr onmouseout="icra19_stop()" onmouseover="icra19_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    
                      <div class="two" id="icra19_image" style="opacity: 0;">
                        <img src="main_page/images/icra19.jpg" width="160">
                      </div>
                      <img src="main_page/images/icra19.jpg" width="160">
                    
                  </div>

                  <script type="text/javascript">
                    function icra19_start() {
                      document.getElementById('icra19_image').style.opacity = 1;
                    }
                    function icra19_stop() {
                      document.getElementById('icra19_image').style.opacity = 0;
                    }
                    optnrt_stop();
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <a href="https://yang-l1.github.io">
                    <papertitle>Pose graph optimization for Unsupervised monocular visual odometry</papertitle>
                  </a>
                  <br>
                  <b>Yang Li</b>,  Yoshitaka Ushiku, and Tatsuya Harada<br>
                  <b>ICRA 2019</b><br>
                  <a href="https://arxiv.org/pdf/1903.06315.pdf">paper</a>
<!--                  <a href="https://www.youtube.com/watch?v=oFgMep7xzrU">video</a> |-->

                  <p> We propose to leverage graph optimization and loop closure detection to overcome  limitations of unsupervised learning based monocular visual odometry.
                  </p>

                </td>
              </tr>




          </tbody></table>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
<!--                <h2>Miscellanea</h2>-->
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>

            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:left;font-size:small;">
                  This website is based on <a href="https://github.com/jonbarron/jonbarron_website">source code</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
